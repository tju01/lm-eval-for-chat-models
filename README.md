This repository describes a proposal for changes in lm-evaluation-harness to integrate benchmarks for instruction-finetuned/chat language models.
While lm-evaluation-harness is often used for these models, the currently implemented benchmarks are primarily designed around base models.
To integrate better support for chat language models, I propose the following changes:
1. [Support for chat templates](chat-templates.md)
2. [Larger focus on zero-shot benchmarks](zero-shot.md)
3. [More flexible tasks](more-flexible-tasks.md)
4. [Inference modifications](inference-modifications.md)
